{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 16:36:56.783084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-14 16:36:56.783325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-14 16:36:56.828033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-14 16:36:56.923524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-14 16:36:58.971028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset(object):\n",
    "    def __init__(self, data_directory: str, file_prefix:str, file_name:str, scaler_prefix:Optional[str], scaler_name:Optional[str]):\n",
    "        self.data_directory = data_directory\n",
    "        self.file_prefix = file_prefix\n",
    "        self.file_name = file_name\n",
    "        self.scaler_prefix = scaler_prefix\n",
    "        self.scaler_name = scaler_name\n",
    "        self.scaler = self.get_scaler()\n",
    "        \n",
    "    def get_scaler(self):\n",
    "        scaler_path = Path() / self.data_directory / self.scaler_prefix / self.scaler_name\n",
    "        try:\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            scaler = None\n",
    "        \n",
    "        if scaler is not None:\n",
    "            self.X_mean, self.X_std = scaler.mean_[:-1], scaler.scale_[:-1]\n",
    "            self.n_inputs = len(scaler.mean_[:-1])\n",
    "        return scaler\n",
    "    \n",
    "    def parse_csv_line(self, line):\n",
    "        defs = [0.] * self.n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "        fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "        return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        x, y = self.parse_csv_line(line)\n",
    "        return (x -self.X_mean) / self.X_std, y\n",
    "        \n",
    "    def csv_reader_dataset(self, n_readers=5, n_read_threads=None, n_parse_threads=5, shuffle_buffer_size=10_000,\n",
    "                          seed=42, batch_size=32):\n",
    "        filepaths = str(Path() / self.data_directory / self.file_prefix / self.file_name)\n",
    "        dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "        dataset = dataset.interleave(\n",
    "            lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "            cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "        dataset = dataset.map(self.preprocess, num_parallel_calls=n_parse_threads)\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "        return dataset.batch(batch_size).prefetch(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../preprocess/data/preprocess\"\n",
    "train_prefix = \"train\"\n",
    "train_file_name = \"housing_train.csv\"\n",
    "valid_prefix = \"valid\"\n",
    "valid_file_name = \"housing_valid.csv\"\n",
    "test_prefix = \"test\"\n",
    "test_file_name = \"housing_test.csv\"\n",
    "\n",
    "scaler_prefix = \"scaler\"\n",
    "scaler_name = \"standard_scaler.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HousingDataset(data_directory=data_directory, file_prefix=train_prefix, file_name=train_file_name, scaler_prefix=scaler_prefix, scaler_name=scaler_name)\n",
    "valid_set = HousingDataset(data_directory=data_directory, file_prefix=valid_prefix, file_name=valid_file_name, scaler_prefix=scaler_prefix, scaler_name=scaler_name)\n",
    "test_set = HousingDataset(data_directory=data_directory, file_prefix=test_prefix, file_name=test_file_name, scaler_prefix=scaler_prefix, scaler_name=scaler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_set.csv_reader_dataset()\n",
    "valid_dataset = valid_set.csv_reader_dataset()\n",
    "test_dataset = test_set.csv_reader_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self, n_outputs=1, **kwargs):\n",
    "        super(SimpleModel, self).__init__(**kwargs)\n",
    "        # self.input = tf.keras.layers.Input(shape=(n_inputs,))\n",
    "        self.fc1 = tf.keras.layers.Dense(units=64, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(units=32, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(units=n_outputs, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: tf.keras.Model,\n",
    "    train_dataset: tf.data.Dataset,\n",
    "    valid_dataset: tf.data.Dataset,\n",
    "    loss_fn,\n",
    "    metrics,\n",
    "    optimizer,\n",
    "    epochs: int = 10,\n",
    "    checkpoints_directory: str = \"/opt/housing/model/\",\n",
    "):\n",
    "    for epcoh in range(epochs):\n",
    "        \n",
    "    model.complie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer,\n",
    "              metrics=[\"RootMeanSquaredError\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 5ms/step - loss: 1.0604 - root_mean_squared_error: 1.0297 - val_loss: 4.6848 - val_root_mean_squared_error: 2.1644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8dbc376f20>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=valid_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            multiple                  576       \n",
      "                                                                 \n",
      " dense_22 (Dense)            multiple                  2080      \n",
      "                                                                 \n",
      " dense_23 (Dense)            multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2689 (10.50 KB)\n",
      "Trainable params: 2689 (10.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
