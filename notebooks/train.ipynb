{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAKAGE_ROOT = Path(os.path.abspath(os.path.dirname(__name__)))\n",
    "sys.path.append(str(PAKAGE_ROOT.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.src.model import HousingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessConfigurations(object):\n",
    "    feature_names = [\n",
    "        \"MedInc\",\n",
    "        \"HouseAge\",\n",
    "        \"AveRooms\",\n",
    "        \"AveBedrms\",\n",
    "        \"Population\",\n",
    "        \"AveOccup\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "    ]\n",
    "    target_names = [\"MedHouseVal\"]\n",
    "    scaler = \"standard\"\n",
    "\n",
    "    train_prefix = \"train\"\n",
    "    train_file_name = \"housing_train.csv\"\n",
    "    valid_prefix = \"valid\"\n",
    "    valid_file_name = \"housing_valid.csv\"\n",
    "    test_prefix = \"test\"\n",
    "    test_file_name = \"housing_test.csv\"\n",
    "    scaler_prefix = \"scaler\"\n",
    "    scaler_name = \"standard_scaler.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../preprocess/data/preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HousingDataset(data_directory=data_directory, file_prefix=PreprocessConfigurations.train_prefix, file_name=PreprocessConfigurations.train_file_name, scaler_prefix=PreprocessConfigurations.scaler_prefix, scaler_name=PreprocessConfigurations.scaler_name)\n",
    "valid_set = HousingDataset(data_directory=data_directory, file_prefix=PreprocessConfigurations.valid_prefix, file_name=PreprocessConfigurations.valid_file_name, scaler_prefix=PreprocessConfigurations.scaler_prefix, scaler_name=PreprocessConfigurations.scaler_name)\n",
    "test_set = HousingDataset(data_directory=data_directory, file_prefix=PreprocessConfigurations.test_prefix, file_name=PreprocessConfigurations.test_file_name, scaler_prefix=PreprocessConfigurations.scaler_prefix, scaler_name=PreprocessConfigurations.scaler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 02:05:04.121935: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-08-16 02:05:04.121955: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-08-16 02:05:04.121959: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-08-16 02:05:04.122073: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-16 02:05:04.122106: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_set.csv_reader_dataset()\n",
    "valid_dataset = valid_set.csv_reader_dataset()\n",
    "test_dataset = test_set.csv_reader_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = len(PreprocessConfigurations.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape:tuple, output_dim:int):\n",
    "    model_input = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Dense(16, activation=\"relu\")(model_input)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    model_output = tf.keras.layers.Dense(output_dim, activation=None)(x)\n",
    "\n",
    "    model = tf.keras.Model(model_input, model_output, name=\"simple_model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                144       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 289 (1.13 KB)\n",
      "Trainable params: 289 (1.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_model(input_shape=(n_inputs, ), output_dim=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "train_metric = tf.keras.metrics.RootMeanSquaredError()\n",
    "val_metric = tf.keras.metrics.RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "valid_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 3.6351\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.9568\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 1.1915\n",
      "Validation rmse: 1.8812\n",
      "Time taken: 9.35s\n",
      "\n",
      " Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.4609\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5616\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.7718\n",
      "Validation rmse: 1.3605\n",
      "Time taken: 4.39s\n",
      "\n",
      " Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.5567\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.4279\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6852\n",
      "Validation rmse: 1.0038\n",
      "Time taken: 4.30s\n",
      "\n",
      " Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.3431\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.4156\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6532\n",
      "Validation rmse: 0.7269\n",
      "Time taken: 4.27s\n",
      "\n",
      " Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.4597\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.3208\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6387\n",
      "Validation rmse: 0.6364\n",
      "Time taken: 4.64s\n",
      "\n",
      " Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.3148\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.5003\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6302\n",
      "Validation rmse: 0.6322\n",
      "Time taken: 4.46s\n",
      "\n",
      " Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.2751\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.2839\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6258\n",
      "Validation rmse: 0.6023\n",
      "Time taken: 4.41s\n",
      "\n",
      " Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.3492\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.2927\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6202\n",
      "Validation rmse: 0.6367\n",
      "Time taken: 4.23s\n",
      "\n",
      " Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.2520\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.4361\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6170\n",
      "Validation rmse: 0.5996\n",
      "Time taken: 4.35s\n",
      "\n",
      " Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.4720\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 200: 0.2501\n",
      "Seen so far: 6432 samples\n",
      "Training rmse over epoch: 0.6118\n",
      "Validation rmse: 0.6410\n",
      "Time taken: 4.15s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\n Start of epoch %d\" % (epoch, ))\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, y_pred)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "\n",
    "        # Update training metric\n",
    "        train_loss(loss_value)\n",
    "        train_metric.update_state(y_batch_train, y_pred)\n",
    "    \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 32))\n",
    "    \n",
    "    train_rmse = train_metric.result()\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('rmse', train_metric.result(), step=epoch)\n",
    "\n",
    "    print(\"Training rmse over epoch: %.4f\" % (float(train_rmse),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metric.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in valid_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_losses = loss_fn(y_batch_val, val_logits)\n",
    "        val_metric.update_state(y_batch_val, val_logits)\n",
    "    \n",
    "    valid_loss(val_losses)\n",
    "    val_rmse = val_metric.result()\n",
    "\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', valid_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('rmse', val_rmse, step=epoch)\n",
    "    val_metric.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    print(\"Validation rmse: %.4f\" % (float(val_rmse),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
